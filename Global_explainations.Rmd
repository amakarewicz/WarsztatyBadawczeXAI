---
title: "Global explainations"
author: "Agata Kaczmarek, Agata Makarewicz, Jacek Wi≈õniewski"
date: "13 05 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(DALEX)
library(DALEXtra)
library(ranger)
library(knitr)
library(lime)
library(gridExtra)
library(dplyr)
set.seed(1)

model_type.dalex_explainer <- DALEXtra::model_type.dalex_explainer
predict_model.dalex_explainer <- DALEXtra::predict_model.dalex_explainer
```

### Data preprocessing

Before performing any explainations of our model's predictions, we needed to conduct simple data preprocessing. Based on EDA, following steps were executed:

* ***handling missing values:*** We identified two features containing missing values, both related to camera parameters (`back_camera_mpix`, `front_camera_mpix`). Those values turned out to be meaningful, as they simply means that given mobile phone has no camera (back or front). Given that information, NA's were imputed with constant value - 0.
* ***removing outliers:*** Based on features distribution, we identified some extreme values in some of them (`back_camera_mpix`, `front_camera_mpix`, `battery_mAh`, `flash_gb`), which would weaken our model's performance.
* ***dealing with unimportant and correlated features:*** We decided to omit `name` variable, because it was practically unique in our dataset and naturally connected to `brand` feature. Moreover, `height_px` and `width_px` were deleted due to their strong correlation with `diag` feature (and with each other); this feature was considered as sufficient determinant of phone's dimensions.
* ***target encoding*** We encoded the only categorical variable left in our data with mean target values for each category (brand), so that we could work only with numerical features, which are easier to process.

```{r data}
phones <- read.csv('./phones.csv')
phones[is.na(phones)] <- 0 # NA means phone does not have camera
phones <- phones[phones$back_camera_mpix < 90 &
                   phones$battery_mAh < 7000 &
                   phones$flash_gb < 400 &
                   phones$front_camera_mpix < 40, ]
phones <- phones[, -c(1, 9, 10)] # removing name, height, width beacause they are useless
phones <- phones %>% group_by(brand) %>% mutate(brand = mean(price)) # target encoding
```

### Model

```{r model, echo=TRUE}
model <- ranger(price~., data = phones, num.trees = 50)
```

```{r explainer,results=FALSE}
explainer <- DALEX::explain(model = model, data = phones[,-8], y = phones$price) 
```

### Global explainations

In the second step of our XAI we focused on dataset level explainations - analysis of all predictions together and how each feature affect their average value. We used Feature Importance, Partial Dependence Profile and Accumulated Local Effect to show dependencies and draw conclusions. The advantage of those methods, in comparison to local profiles, is the fact that those explainations are true in general for our data; whereas for each observation results can differ greatly. Additionaly, we checked popular regression metrics values for our model in order to evaluate its performance.

### Metrics

We decided to assess our model's performance with 4 common regression metrics:

* Mean Squared Error
* Root Mean Squared Error
* R2 score
* Mean Absolute Error

```{r ranger_performance, echo=FALSE}
measures <- data.frame(model_performance(explainer)$measures)
names(measures) <- c('mse','rmse','r2','mae')
kable(measures)
```

Given the values of the metrics above, we can conclude that our model performs well - R2 score above 0.95 and RMSE around 300 are satisfying.

### Feature importance

```{r plot_rf, echo=FALSE}
fi_rf <- model_parts(explainer, B = 10)
plot(fi_rf)
```

The most important variables for our model are `ram_gb`, `flash_gb` and, depending on the permutation, `brand` or `front_camera_mpix`. However, all those 4 variables are similar in terms of importance and they differ (not significantly but still) from the rest of the features. These conclusions overlap with those we drew from local explainations - for selected samples analyzed in previous section, storage related variables and `brand` had great impact on our prediction. In general, it means that it is storage size, brand and camera quality that influence mobile phone price the most.

### Partial Dependence Profile

```{r}
pdp <- model_profile(explainer, variables = c('brand', 'ram_gb', 'flash_gb', 'front_camera_mpix'))
plot(pdp, geom = "profiles")
```

```{r}
pdp_1 <- model_profile(explainer, variables = c('brand', 'ram_gb', 'flash_gb', 'front_camera_mpix'), N = 1000, grid_points = 100)
pdp_2 <- model_profile(explainer, variables = c('brand', 'ram_gb', 'flash_gb', 'front_camera_mpix'), N = 100, grid_points = 100)
pdp_1$agr_profiles$`_label_` <- "1000 observations"
pdp_2$agr_profiles$`_label_` <- "100 observations"
plot(pdp_1, pdp_2)
```

We have created PDP plots for 4 most important variables for the model. As we can observe, as the variable grows, so does the price increase. The only exception is the `front_camera_mpix` column.

Additionally, we have decided to compare results of the PDP depanding on number of observations. We realized that PDP with more observations gives higher price forecasts, especially when we talk about lower prices.


### Accumulated Local Dependence 

```{r}
ale_1 <- model_profile(explainer,variables = c('brand', 'ram_gb', 'flash_gb', 'front_camera_mpix'), N = 1000, grid_points = 100, type = "accumulated")
ale_2 <- model_profile(explainer,variables = c('brand', 'ram_gb', 'flash_gb', 'front_camera_mpix'), N = 100, grid_points = 100, type = "accumulated")

ale_1$agr_profiles$`_label_` <- "1000 observations"
ale_2$agr_profiles$`_label_` <- "100 observations"
plot(ale_1, ale_2)
```

Here we have created ALE plots for 4 most important variables for the model. They are quite similar to the PDP ones, also grow for *brand*, *flash_gb* and *ram_gb*. This time it even a little bit grows for *front_camera_mpix* but only in some intervals.

This time, in most cases, for less observations we see slightly higher priedictions.

### PDP vs ALE

```{r}
ale_1$agr_profiles$`_label_` <- "ALE"
pdp_1$agr_profiles$`_label_` <- "PDP"
plot(ale_1, pdp_1)
```

There are some differences between PDP and ALE, for some features (*front_camera_mpix* and *ram_gb*) PDP predicts slightly higher price, for other features ALE predicts higher.But in general, the shape of chart for feature between PDP and ALE remains the same. 